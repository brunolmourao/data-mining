{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on!\n",
    "\n",
    "Nessa prática, sugerimos alguns pequenos exemplos para você implementar sobre o Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimar o Pi\n",
    "\n",
    "Existe um algoritmo para estimar o Pi com números radômicos. Implemente-o sobre o Spark.\n",
    "\n",
    "Descrição do algoritmo: http://www.eveandersson.com/pi/monte-carlo-circle\n",
    "\n",
    "Implementação EM PYTHON (__não sobre o SPARK__): http://www.stealthcopter.com/blog/2009/09/python-calculating-pi-using-random-numbers/\n",
    "\n",
    "O númer de pontos deve ser 100000 (cem mill) vezes o número mínimo de partições padrão do seu SparkContext (`sc.defaultMinPartitions`). Esses pontos devem ser selecionados aleatóriamente na etapa de map (ver observações).\n",
    "\n",
    "Observações: use as funções __map__ (para mapear as ocorrêncas em `0` ou `1`, significando `1` quando o ponto aleatório cair dentro do círculo e `0` quando o contrário) e __reduce__ (para sumar as ocorrências)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.164\n"
     ]
    }
   ],
   "source": [
    "from random import *\n",
    "from math import sqrt\n",
    "inside=0\n",
    "n=1000\n",
    "for i in range(0,n):\n",
    "    x=random()\n",
    "    y=random()\n",
    "    if sqrt(x*x+y*y)<=1:\n",
    "        inside+=1\n",
    "pi=4*inside/n\n",
    "\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "from math import sqrt\n",
    "def soma(a,b): return a+b\n",
    "def area(x,y):return 1 if sqrt(x*x+y*y)<=1 else  0\n",
    "def mapfunction(z):\n",
    "    x=random()\n",
    "    y=random()\n",
    "    return area(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nummin = sc.defaultMinPartitions * 10000\n",
    "data = sc.parallelize(range(1, nummin))\n",
    "res1 = data.map(mapfunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15767\n"
     ]
    }
   ],
   "source": [
    "print(res1.reduce(soma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.153557677883894\n"
     ]
    }
   ],
   "source": [
    "pispark = 4*(res1.reduce(soma))/(res1.count())\n",
    "print(pispark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtragem de Primos\n",
    "\n",
    "Dado uma sequência de números de `1` a `1000000`, filtre somente os primos dessa sequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primes( n ):\n",
    "# i sera nosso divisor inicial\n",
    "    i = 1;\n",
    "# j sera nosso contador de ocorrências\n",
    "    j = 0;\n",
    "#Nenhum numero real vai ser divisivel por um numero maior do que sua metade\n",
    "    n1 = (n/2);\n",
    "\n",
    "    while (i <= n):\n",
    "\n",
    "       if (n % i==0):\n",
    "          i = i+1;\n",
    "          j = j+1;\n",
    "\n",
    "       if (i>=n1):\n",
    "        # damos a i, o valor da variavel entrada, pois o próximo divisor sera o próprio número\n",
    "          i = n;\n",
    "          i = i+1;\n",
    "          j = j+1;\n",
    "\n",
    "       else:\n",
    "          i = i+1;\n",
    "    if(j==2):\n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(range(1, 1000000))\n",
    "res = data.filter(primes)\n",
    "print(res.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Municípios do Brasil\n",
    "\n",
    "Dado o dataset `mucipios_do_Brasil.csv`, faça duas operações com ele:\n",
    "\n",
    "1. Monte uma lista dos municípios por estado.\n",
    "2. Conte quantos municípios há em cada estado.\n",
    "\n",
    "Dicas: use as operações groupByKey e reduceByKey, não faça um count na lista da operação 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['uf', 'cidade', 'lat', 'lng', 'cond'], ['AC', 'Acrelândia', '-9.825808', '-66.897166', 'false']]\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"municipios_do_Brasil.csv\")\n",
    "\n",
    "data = lines.map(lambda line: line.split(\",\"))\n",
    "\n",
    "print(data.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-74e71d1e-d799-47f4-8575-d4ff8340c44f/pyspark_runner.py\", line 188, in <module>\n",
       "    ast_parsed = ast.parse(final_code)\n",
       "  File \"/home/bruno/miniconda3/envs/spark/lib/python3.6/ast.py\", line 35, in parse\n",
       "    return compile(source, filename, mode, PyCF_ONLY_AST)\n",
       "  File \"<unknown>\", line 1\n",
       "    data.groupByKey(lambda uf,cidade)\n",
       "                                    ^\n",
       "SyntaxError: invalid syntax\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupByKey(lambda uf,cidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-74e71d1e-d799-47f4-8575-d4ff8340c44f/pyspark_runner.py\", line 194, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 1, in <module>\n",
       "  File \"/home/bruno/spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\", line 1020, in __getattr__\n",
       "    \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n",
       "AttributeError: 'DataFrame' object has no attribute 'groupByKey'\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupByKey(\"uf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word Count Memória Postumas de Brás Cubas\n",
    "\n",
    "Memórias Póstumas de Brás Cubas é um romance escrito por Machado de Assis, desenvolvido em princípio como folhetim, de março a dezembro de 1880, na Revista Brasileira, para, no ano seguinte, ser publicado como livro, pela então Tipografia Nacional.\n",
    "\n",
    "A obra retrata a escravidão, as classes sociais, o cientificismo e o positivismo da época. Dada essas informações, será que conseguimos idenficar essas características pelas palavras mais utilizadas em sua obra?\n",
    "\n",
    "Utilizando o dataset `Machado-de-Assis-Memorias-Postumas.txt`, faça um word count e encontre as palavras mais utilizadas por Machado de Assis em sua obra. Não esqueça de utilizar `stopwords.pt` para remover as `stop words`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "3.6.3\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
